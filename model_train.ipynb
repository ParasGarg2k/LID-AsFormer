{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde95e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cair/miniconda3/envs/videomaev2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Modular loss function initialized with: λ1=1.0, λ2=0.15, τ=4.0\n",
      "Model initialized with 4 attention heads.\n",
      "Model Size: 2,735,980\n",
      "LR: 0.0005\n",
      "[epoch 1]: loss = 2.4093, acc = 0.1083\n",
      "[epoch 2]: loss = 2.0279, acc = 0.1331\n",
      "[epoch 3]: loss = 1.8290, acc = 0.1538\n",
      "[epoch 4]: loss = 1.6522, acc = 0.1733\n",
      "[epoch 5]: loss = 1.5604, acc = 0.1905\n",
      "---[epoch 5]---: tst acc = 0.2460\n",
      "[epoch 6]: loss = 1.4667, acc = 0.2117\n",
      "[epoch 7]: loss = 1.4031, acc = 0.2317\n",
      "[epoch 8]: loss = 1.3389, acc = 0.2687\n",
      "[epoch 9]: loss = 1.2644, acc = 0.3106\n",
      "[epoch 10]: loss = 1.2230, acc = 0.3448\n",
      "---[epoch 10]---: tst acc = 0.3669\n",
      "[epoch 11]: loss = 1.1730, acc = 0.3680\n",
      "[epoch 12]: loss = 1.1395, acc = 0.3882\n",
      "[epoch 13]: loss = 1.0777, acc = 0.4178\n",
      "[epoch 14]: loss = 1.0217, acc = 0.4445\n",
      "[epoch 15]: loss = 0.9882, acc = 0.4708\n",
      "---[epoch 15]---: tst acc = 0.4632\n",
      "[epoch 16]: loss = 0.9452, acc = 0.4949\n",
      "[epoch 17]: loss = 0.9285, acc = 0.5209\n",
      "[epoch 18]: loss = 0.8988, acc = 0.5480\n",
      "[epoch 19]: loss = 0.8575, acc = 0.5853\n",
      "[epoch 20]: loss = 0.8162, acc = 0.6127\n",
      "---[epoch 20]---: tst acc = 0.5883\n",
      "[epoch 21]: loss = 0.8004, acc = 0.6279\n",
      "[epoch 22]: loss = 0.7733, acc = 0.6458\n",
      "[epoch 23]: loss = 0.7256, acc = 0.6780\n",
      "[epoch 24]: loss = 0.7102, acc = 0.6925\n",
      "[epoch 25]: loss = 0.7044, acc = 0.7047\n",
      "---[epoch 25]---: tst acc = 0.6629\n",
      "[epoch 26]: loss = 0.6664, acc = 0.7193\n",
      "[epoch 27]: loss = 0.6556, acc = 0.7255\n",
      "[epoch 28]: loss = 0.6430, acc = 0.7383\n",
      "[epoch 29]: loss = 0.6266, acc = 0.7463\n",
      "[epoch 30]: loss = 0.5930, acc = 0.7632\n",
      "---[epoch 30]---: tst acc = 0.7220\n",
      "[epoch 31]: loss = 0.5900, acc = 0.7681\n",
      "[epoch 32]: loss = 0.5729, acc = 0.7829\n",
      "[epoch 33]: loss = 0.5525, acc = 0.7831\n",
      "[epoch 34]: loss = 0.5423, acc = 0.8012\n",
      "[epoch 35]: loss = 0.5234, acc = 0.8029\n",
      "---[epoch 35]---: tst acc = 0.7334\n",
      "[epoch 36]: loss = 0.5029, acc = 0.8178\n",
      "[epoch 37]: loss = 0.5043, acc = 0.8126\n",
      "[epoch 38]: loss = 0.4791, acc = 0.8273\n",
      "[epoch 39]: loss = 0.4904, acc = 0.8252\n",
      "[epoch 40]: loss = 0.4636, acc = 0.8274\n",
      "---[epoch 40]---: tst acc = 0.7283\n",
      "[epoch 41]: loss = 0.4587, acc = 0.8415\n",
      "[epoch 42]: loss = 0.4604, acc = 0.8436\n",
      "[epoch 43]: loss = 0.4373, acc = 0.8449\n",
      "[epoch 44]: loss = 0.4321, acc = 0.8514\n",
      "[epoch 45]: loss = 0.4378, acc = 0.8516\n",
      "---[epoch 45]---: tst acc = 0.7329\n",
      "[epoch 46]: loss = 0.4236, acc = 0.8537\n",
      "[epoch 47]: loss = 0.4167, acc = 0.8568\n",
      "[epoch 48]: loss = 0.3970, acc = 0.8673\n",
      "[epoch 49]: loss = 0.3963, acc = 0.8665\n",
      "[epoch 50]: loss = 0.3855, acc = 0.8767\n",
      "---[epoch 50]---: tst acc = 0.7659\n",
      "[epoch 51]: loss = 0.3733, acc = 0.8754\n",
      "[epoch 52]: loss = 0.3845, acc = 0.8767\n",
      "[epoch 53]: loss = 0.3799, acc = 0.8815\n",
      "[epoch 54]: loss = 0.3814, acc = 0.8796\n",
      "[epoch 55]: loss = 0.3690, acc = 0.8858\n",
      "---[epoch 55]---: tst acc = 0.7621\n",
      "[epoch 56]: loss = 0.3673, acc = 0.8878\n",
      "[epoch 57]: loss = 0.3496, acc = 0.8918\n",
      "[epoch 58]: loss = 0.3466, acc = 0.8879\n",
      "[epoch 59]: loss = 0.3361, acc = 0.8942\n",
      "[epoch 60]: loss = 0.3347, acc = 0.8993\n",
      "---[epoch 60]---: tst acc = 0.7823\n",
      "[epoch 61]: loss = 0.3338, acc = 0.9028\n",
      "[epoch 62]: loss = 0.3403, acc = 0.8960\n",
      "[epoch 63]: loss = 0.3232, acc = 0.8957\n",
      "[epoch 64]: loss = 0.3101, acc = 0.9102\n",
      "[epoch 65]: loss = 0.3160, acc = 0.9073\n",
      "---[epoch 65]---: tst acc = 0.7819\n",
      "[epoch 66]: loss = 0.3091, acc = 0.9109\n",
      "[epoch 67]: loss = 0.3082, acc = 0.9081\n",
      "[epoch 68]: loss = 0.3002, acc = 0.9075\n",
      "[epoch 69]: loss = 0.3022, acc = 0.9113\n",
      "[epoch 70]: loss = 0.3048, acc = 0.9149\n",
      "---[epoch 70]---: tst acc = 0.7983\n",
      "[epoch 71]: loss = 0.2952, acc = 0.9111\n",
      "[epoch 72]: loss = 0.2937, acc = 0.9117\n",
      "[epoch 73]: loss = 0.2849, acc = 0.9171\n",
      "[epoch 74]: loss = 0.2979, acc = 0.9110\n",
      "[epoch 75]: loss = 0.2863, acc = 0.9174\n",
      "---[epoch 75]---: tst acc = 0.7707\n",
      "[epoch 76]: loss = 0.2827, acc = 0.9149\n",
      "[epoch 77]: loss = 0.2814, acc = 0.9174\n",
      "[epoch 78]: loss = 0.2744, acc = 0.9201\n",
      "[epoch 79]: loss = 0.2837, acc = 0.9151\n",
      "[epoch 80]: loss = 0.2664, acc = 0.9242\n",
      "---[epoch 80]---: tst acc = 0.7889\n",
      "[epoch 81]: loss = 0.2655, acc = 0.9256\n",
      "[epoch 82]: loss = 0.2654, acc = 0.9241\n",
      "[epoch 83]: loss = 0.2655, acc = 0.9198\n",
      "[epoch 84]: loss = 0.2598, acc = 0.9262\n",
      "[epoch 85]: loss = 0.2613, acc = 0.9248\n",
      "---[epoch 85]---: tst acc = 0.7904\n",
      "[epoch 86]: loss = 0.2498, acc = 0.9322\n",
      "[epoch 87]: loss = 0.2532, acc = 0.9275\n",
      "[epoch 88]: loss = 0.2518, acc = 0.9271\n",
      "[epoch 89]: loss = 0.2494, acc = 0.9275\n",
      "[epoch 90]: loss = 0.2506, acc = 0.9255\n",
      "---[epoch 90]---: tst acc = 0.7977\n",
      "[epoch 91]: loss = 0.2472, acc = 0.9271\n",
      "[epoch 92]: loss = 0.2552, acc = 0.9235\n",
      "[epoch 93]: loss = 0.2439, acc = 0.9282\n",
      "[epoch 94]: loss = 0.2543, acc = 0.9262\n",
      "[epoch 95]: loss = 0.2417, acc = 0.9310\n",
      "---[epoch 95]---: tst acc = 0.8005\n",
      "[epoch 96]: loss = 0.2397, acc = 0.9324\n",
      "[epoch 97]: loss = 0.2352, acc = 0.9331\n",
      "[epoch 98]: loss = 0.2398, acc = 0.9334\n",
      "[epoch 99]: loss = 0.2305, acc = 0.9333\n",
      "[epoch 100]: loss = 0.2249, acc = 0.9365\n",
      "---[epoch 100]---: tst acc = 0.7977\n",
      "[epoch 101]: loss = 0.2294, acc = 0.9321\n",
      "[epoch 102]: loss = 0.2317, acc = 0.9338\n",
      "[epoch 103]: loss = 0.2240, acc = 0.9346\n",
      "[epoch 104]: loss = 0.2285, acc = 0.9358\n",
      "[epoch 105]: loss = 0.2242, acc = 0.9396\n",
      "---[epoch 105]---: tst acc = 0.7863\n",
      "[epoch 106]: loss = 0.2256, acc = 0.9366\n",
      "[epoch 107]: loss = 0.2153, acc = 0.9429\n",
      "[epoch 108]: loss = 0.2158, acc = 0.9387\n",
      "[epoch 109]: loss = 0.2277, acc = 0.9370\n",
      "[epoch 110]: loss = 0.2177, acc = 0.9436\n",
      "---[epoch 110]---: tst acc = 0.7972\n",
      "Epoch 00111: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[epoch 111]: loss = 0.2200, acc = 0.9408\n",
      "[epoch 112]: loss = 0.2097, acc = 0.9423\n",
      "[epoch 113]: loss = 0.2075, acc = 0.9464\n",
      "[epoch 114]: loss = 0.2121, acc = 0.9412\n",
      "[epoch 115]: loss = 0.2133, acc = 0.9395\n",
      "---[epoch 115]---: tst acc = 0.8047\n",
      "[epoch 116]: loss = 0.2092, acc = 0.9412\n",
      "Epoch 00117: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[epoch 117]: loss = 0.2099, acc = 0.9432\n",
      "[epoch 118]: loss = 0.2072, acc = 0.9403\n",
      "[epoch 119]: loss = 0.2109, acc = 0.9423\n",
      "[epoch 120]: loss = 0.2033, acc = 0.9447\n",
      "---[epoch 120]---: tst acc = 0.8060\n",
      "[epoch 121]: loss = 0.2023, acc = 0.9466\n",
      "[epoch 122]: loss = 0.2062, acc = 0.9468\n",
      "[epoch 123]: loss = 0.2043, acc = 0.9451\n",
      "[epoch 124]: loss = 0.2063, acc = 0.9450\n",
      "Epoch 00125: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[epoch 125]: loss = 0.2158, acc = 0.9439\n",
      "---[epoch 125]---: tst acc = 0.7920\n",
      "[epoch 126]: loss = 0.2022, acc = 0.9465\n",
      "[epoch 127]: loss = 0.2033, acc = 0.9461\n",
      "[epoch 128]: loss = 0.1990, acc = 0.9464\n",
      "[epoch 129]: loss = 0.1980, acc = 0.9456\n",
      "[epoch 130]: loss = 0.2018, acc = 0.9435\n",
      "---[epoch 130]---: tst acc = 0.7983\n",
      "[epoch 131]: loss = 0.2001, acc = 0.9468\n",
      "[epoch 132]: loss = 0.2005, acc = 0.9478\n",
      "Epoch 00133: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[epoch 133]: loss = 0.2034, acc = 0.9465\n",
      "[epoch 134]: loss = 0.2074, acc = 0.9492\n",
      "[epoch 135]: loss = 0.2017, acc = 0.9463\n",
      "---[epoch 135]---: tst acc = 0.7999\n",
      "[epoch 136]: loss = 0.2079, acc = 0.9454\n",
      "Epoch 00137: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[epoch 137]: loss = 0.1985, acc = 0.9466\n",
      "[epoch 138]: loss = 0.1999, acc = 0.9507\n",
      "[epoch 139]: loss = 0.1970, acc = 0.9487\n",
      "[epoch 140]: loss = 0.1988, acc = 0.9470\n",
      "---[epoch 140]---: tst acc = 0.8021\n",
      "[epoch 141]: loss = 0.1980, acc = 0.9463\n",
      "[epoch 142]: loss = 0.2037, acc = 0.9455\n",
      "Epoch 00143: reducing learning rate of group 0 to 7.8125e-06.\n",
      "[epoch 143]: loss = 0.2009, acc = 0.9470\n",
      "[epoch 144]: loss = 0.1985, acc = 0.9486\n",
      "[epoch 145]: loss = 0.2007, acc = 0.9489\n",
      "---[epoch 145]---: tst acc = 0.8034\n",
      "[epoch 146]: loss = 0.2052, acc = 0.9453\n",
      "Epoch 00147: reducing learning rate of group 0 to 3.9063e-06.\n",
      "[epoch 147]: loss = 0.1984, acc = 0.9459\n",
      "[epoch 148]: loss = 0.1998, acc = 0.9468\n",
      "[epoch 149]: loss = 0.1992, acc = 0.9482\n",
      "[epoch 150]: loss = 0.2002, acc = 0.9502\n",
      "---[epoch 150]---: tst acc = 0.8032\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "\n",
    "from trainer import Trainer  # Import the Trainer class from the file you have edited\n",
    "from model import * # Import the model from the file you have edited\n",
    "from utils import BatchGenerator\n",
    "from utils import func_eval\n",
    "\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Using device:\", device)\n",
    "seed = 19988563\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    " \n",
    "action = 'train'\n",
    "dataset = 'gtea'\n",
    "split = '1'\n",
    " \n",
    "num_epochs = 150 # number of epochs to train\n",
    "\n",
    "lr = 0.0005\n",
    "num_layers = 9 # number of layers in the one encoder or decoder block\n",
    "num_f_maps = 64\n",
    "features_dim = 2048 # dimension of the input features, 2048 for i3d features\n",
    "bz = 8 # batch size, 1 for gtea\n",
    " \n",
    "channel_mask_rate = 0.3\n",
    "\n",
    "\n",
    "# use the full temporal resolution @ 15fps\n",
    "sample_rate = 2\n",
    "# sample input features @ 15fps instead of 30 fps\n",
    "# for 50salads, and up-sample the output to 30 fps\n",
    "if dataset == \"50salads\":\n",
    "    sample_rate = 2\n",
    "\n",
    "# To prevent over-fitting for GTEA. Early stopping & large dropout rate\n",
    "if dataset == \"gtea\":\n",
    "    channel_mask_rate = 0.5\n",
    "    \n",
    "if dataset == 'breakfast':\n",
    "    lr = 0.0001\n",
    "\n",
    "# no need to change the following paths if you are using the same dataset\n",
    "vid_list_file = \"files_data/train.split1.bundle\"\n",
    "vid_list_file_tst = \"files_data/test.split1.bundle\"\n",
    "features_path = \"data_i3d/gtea/features\"\n",
    "gt_path = \"data_i3d/gtea/groundTruth\"\n",
    "\n",
    "mapping_file = \"mapping.txt\"\n",
    " # If you have made changes to the model, change the model_dir and results_dir paths accordingly.\n",
    "model_dir = \"results/\"+dataset+\"/split_\"+split\n",
    "results_dir = \"results/\"\n",
    " \n",
    "\n",
    " \n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    " \n",
    " \n",
    "file_ptr = open(mapping_file, 'r')\n",
    "actions = file_ptr.read().split('\\n')[:-1]\n",
    "file_ptr.close()\n",
    "actions_dict = dict()\n",
    "for a in actions:\n",
    "    actions_dict[a.split()[1]] = int(a.split()[0])\n",
    "index2label = dict()\n",
    "for k,v in actions_dict.items():\n",
    "    index2label[v] = k\n",
    "num_classes = len(actions_dict)\n",
    "\n",
    "# Create the model and trainer\n",
    "trainer = Trainer(num_layers, 2, 2, num_f_maps, features_dim, num_classes, channel_mask_rate)\n",
    "\n",
    "# By running this cell, you can train the model.\n",
    "if action == \"train\":\n",
    "    batch_gen = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate, features_dim)\n",
    "    batch_gen.read_data(vid_list_file)\n",
    "\n",
    "    batch_gen_tst = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate, features_dim)\n",
    "    batch_gen_tst.read_data(vid_list_file_tst)\n",
    "\n",
    "    trainer.train(model_dir, batch_gen, num_epochs, bz, lr, batch_gen_tst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902c8b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed in 12.88 seconds\n"
     ]
    }
   ],
   "source": [
    "# By running the following cell, you can predict the results on the test set. The results will be saved in the results_dir.\n",
    "# You can then evaluate the results using the eval.ipynb script.\n",
    "action = \"predict\"\n",
    "if action == \"predict\":\n",
    "    batch_gen_tst = BatchGenerator(num_classes, actions_dict, gt_path, features_path, sample_rate, features_dim)\n",
    "    batch_gen_tst.read_data(vid_list_file_tst)\n",
    "    trainer.predict(model_dir, results_dir, features_path, batch_gen_tst, num_epochs, actions_dict, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81175247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videomaev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
